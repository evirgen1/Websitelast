<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Mathematics for Data Analysis | Yusuf Evirgen</title>
<meta name=keywords content="mathematics,statistics,linear-algebra"><meta name=description content="Essential mathematical concepts for data analysis and machine learning"><meta name=author content="Yusuf Evirgen"><link rel=canonical href=https://yourname.github.io/teaching/mathematics-module/><link crossorigin=anonymous href=/assets/css/stylesheet.b8a2c688bac0aa678bc514c979d665932e05c4ecba77147c206c16b7da0192f5.css integrity="sha256-uKLGiLrAqmeLxRTJedZlky4FxOy6dxR8IGwWt9oBkvU=" rel="preload stylesheet" as=style><link rel=icon href=https://yourname.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yourname.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yourname.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://yourname.github.io/apple-touch-icon.png><link rel=mask-icon href=https://yourname.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yourname.github.io/teaching/mathematics-module/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://yourname.github.io/teaching/mathematics-module/"><meta property="og:site_name" content="Yusuf Evirgen"><meta property="og:title" content="Mathematics for Data Analysis"><meta property="og:description" content="Essential mathematical concepts for data analysis and machine learning"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="teaching"><meta property="article:published_time" content="2023-01-15T00:00:00+00:00"><meta property="article:modified_time" content="2023-01-15T00:00:00+00:00"><meta property="article:tag" content="Mathematics"><meta property="article:tag" content="Statistics"><meta property="article:tag" content="Linear-Algebra"><meta name=twitter:card content="summary"><meta name=twitter:title content="Mathematics for Data Analysis"><meta name=twitter:description content="Essential mathematical concepts for data analysis and machine learning"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Teaching","item":"https://yourname.github.io/teaching/"},{"@type":"ListItem","position":2,"name":"Mathematics for Data Analysis","item":"https://yourname.github.io/teaching/mathematics-module/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Mathematics for Data Analysis","name":"Mathematics for Data Analysis","description":"Essential mathematical concepts for data analysis and machine learning","keywords":["mathematics","statistics","linear-algebra"],"articleBody":"Mathematics for Data Analysis This module covers essential mathematical concepts that form the foundation of modern data analysis, statistics, and machine learning.\nLearning Objectives By the end of this module, students will be able to:\nApply linear algebra concepts to data analysis problems Understand fundamental concepts in probability theory Apply calculus techniques for optimization problems Implement numerical methods for data analysis Linear Algebra Review Vector Spaces A vector space $V$ over a field $F$ consists of a set of vectors and two operations: vector addition and scalar multiplication, satisfying certain axioms.\nFor data analysis, we typically work with the vector space $\\mathbb{R}^n$, which consists of all $n$-dimensional vectors with real entries.\nMatrix Operations For matrices $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}$, the matrix product $C = AB$ is defined as:\n$$C_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj}$$\nEigenvalues and Eigenvectors For a square matrix $A \\in \\mathbb{R}^{n \\times n}$, a scalar $\\lambda$ and a non-zero vector $v \\in \\mathbb{R}^n$ are called an eigenvalue and eigenvector of $A$ if:\n$$Av = \\lambda v$$\nFinding eigenvalues and eigenvectors is crucial for dimensionality reduction techniques like PCA.\nProbability Theory Random Variables A random variable $X$ is a function that maps outcomes from a sample space to real numbers. The probability distribution of $X$ describes the likelihood of different values.\nExpected Value and Variance For a discrete random variable $X$ with probability mass function $p(x)$, the expected value is:\n$$E[X] = \\sum_{x} x \\cdot p(x)$$\nThe variance is:\n$$\\text{Var}(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2$$\nBayes’ Theorem Bayes’ theorem relates conditional probabilities:\n$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\nThis forms the foundation of many machine learning algorithms.\nCalculus for Optimization Gradients The gradient of a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is the vector of partial derivatives:\n$$\\nabla f(x) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n} \\right)$$\nOptimization Finding the minimum of a function $f$ often involves setting the gradient to zero:\n$$\\nabla f(x) = 0$$\nThis is the basis for many machine learning algorithms like gradient descent:\n$$x_{t+1} = x_t - \\alpha \\nabla f(x_t)$$\nwhere $\\alpha$ is the learning rate.\nCode Example: Principal Component Analysis Here’s a Python implementation of PCA using eigenvalue decomposition:\nimport numpy as np import matplotlib.pyplot as plt def pca(X, num_components): \"\"\" Perform PCA on dataset X Parameters: X: Dataset, shape (n_samples, n_features) num_components: Number of principal components to keep Returns: X_pca: Data transformed into principal components components: Principal components explained_variance: Explained variance of each component \"\"\" # Center the data X_centered = X - np.mean(X, axis=0) # Compute covariance matrix cov_matrix = np.cov(X_centered, rowvar=False) # Eigenvalue decomposition eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix) # Sort eigenvalues and corresponding eigenvectors in descending order idx = np.argsort(eigenvalues)[::-1] eigenvalues = eigenvalues[idx] eigenvectors = eigenvectors[:, idx] # Select top k eigenvectors components = eigenvectors[:, :num_components] # Compute explained variance explained_variance = eigenvalues[:num_components] / np.sum(eigenvalues) # Project data onto principal components X_pca = np.dot(X_centered, components) return X_pca, components, explained_variance # Example usage if __name__ == \"__main__\": # Generate random data np.random.seed(42) X = np.random.randn(100, 5) # Apply PCA X_pca, components, explained_variance = pca(X, num_components=2) # Print results print(f\"Explained variance: {explained_variance}\") print(f\"Principal components shape: {components.shape}\") # Plot results plt.figure(figsize=(8, 6)) plt.scatter(X_pca[:, 0], X_pca[:, 1]) plt.xlabel(\"First Principal Component\") plt.ylabel(\"Second Principal Component\") plt.title(\"PCA Example\") plt.grid(True) plt.show() Assignment Implement the following algorithms using the mathematical concepts covered in this module:\nLinear regression using normal equations: $\\hat{\\beta} = (X^T X)^{-1} X^T y$ k-means clustering Naive Bayes classifier using Bayes’ theorem Resources Textbook: “Mathematics for Machine Learning” by Deisenroth, Faisal, and Ong Online course: MIT OpenCourseWare 18.06 Linear Algebra Python libraries: NumPy, SciPy, scikit-learn ","wordCount":"601","inLanguage":"en","datePublished":"2023-01-15T00:00:00Z","dateModified":"2023-01-15T00:00:00Z","author":{"@type":"Person","name":"Yusuf Evirgen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yourname.github.io/teaching/mathematics-module/"},"publisher":{"@type":"Organization","name":"Yusuf Evirgen","logo":{"@type":"ImageObject","url":"https://yourname.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yourname.github.io/ accesskey=h title="Yusuf Evirgen (Alt + H)">Yusuf Evirgen</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yourname.github.io/ title=About><span>About</span></a></li><li><a href=https://yourname.github.io/teaching/ title=Teaching><span>Teaching</span></a></li><li><a href=https://yourname.github.io/research/ title=Research><span>Research</span></a></li><li><a href=https://yourname.github.io/data/ title=Data><span>Data</span></a></li><li><a href=https://yourname.github.io/public-appearances/ title="Public Appearances"><span>Public Appearances</span></a></li><li><a href=https://yourname.github.io/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=course-page><header class=course-header><h1 class=course-title>Mathematics for Data Analysis</h1><div class=course-meta><div class=course-detail><span class=detail-label>Semester:</span>
<span class="detail-value semester">Spring 2023</span></div><div class=course-detail><span class=detail-label>Level:</span>
<span class="detail-value level">Graduate</span></div></div><div class=course-links></div><div class=course-summary>Essential mathematical concepts for data analysis and machine learning</div></header><div class=course-content><h1 id=mathematics-for-data-analysis>Mathematics for Data Analysis</h1><p>This module covers essential mathematical concepts that form the foundation of modern data analysis, statistics, and machine learning.</p><h2 id=learning-objectives>Learning Objectives</h2><p>By the end of this module, students will be able to:</p><ul><li>Apply linear algebra concepts to data analysis problems</li><li>Understand fundamental concepts in probability theory</li><li>Apply calculus techniques for optimization problems</li><li>Implement numerical methods for data analysis</li></ul><h2 id=linear-algebra-review>Linear Algebra Review</h2><h3 id=vector-spaces>Vector Spaces</h3><p>A vector space $V$ over a field $F$ consists of a set of vectors and two operations: vector addition and scalar multiplication, satisfying certain axioms.</p><p>For data analysis, we typically work with the vector space $\mathbb{R}^n$, which consists of all $n$-dimensional vectors with real entries.</p><h3 id=matrix-operations>Matrix Operations</h3><p>For matrices $A \in \mathbb{R}^{m \times n}$ and $B \in \mathbb{R}^{n \times p}$, the matrix product $C = AB$ is defined as:</p><p>$$C_{ij} = \sum_{k=1}^{n} A_{ik} B_{kj}$$</p><h3 id=eigenvalues-and-eigenvectors>Eigenvalues and Eigenvectors</h3><p>For a square matrix $A \in \mathbb{R}^{n \times n}$, a scalar $\lambda$ and a non-zero vector $v \in \mathbb{R}^n$ are called an eigenvalue and eigenvector of $A$ if:</p><p>$$Av = \lambda v$$</p><p>Finding eigenvalues and eigenvectors is crucial for dimensionality reduction techniques like PCA.</p><h2 id=probability-theory>Probability Theory</h2><h3 id=random-variables>Random Variables</h3><p>A random variable $X$ is a function that maps outcomes from a sample space to real numbers. The probability distribution of $X$ describes the likelihood of different values.</p><h3 id=expected-value-and-variance>Expected Value and Variance</h3><p>For a discrete random variable $X$ with probability mass function $p(x)$, the expected value is:</p><p>$$E[X] = \sum_{x} x \cdot p(x)$$</p><p>The variance is:</p><p>$$\text{Var}(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2$$</p><h3 id=bayes-theorem>Bayes&rsquo; Theorem</h3><p>Bayes&rsquo; theorem relates conditional probabilities:</p><p>$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$</p><p>This forms the foundation of many machine learning algorithms.</p><h2 id=calculus-for-optimization>Calculus for Optimization</h2><h3 id=gradients>Gradients</h3><p>The gradient of a function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is the vector of partial derivatives:</p><p>$$\nabla f(x) = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n} \right)$$</p><h3 id=optimization>Optimization</h3><p>Finding the minimum of a function $f$ often involves setting the gradient to zero:</p><p>$$\nabla f(x) = 0$$</p><p>This is the basis for many machine learning algorithms like gradient descent:</p><p>$$x_{t+1} = x_t - \alpha \nabla f(x_t)$$</p><p>where $\alpha$ is the learning rate.</p><h2 id=code-example-principal-component-analysis>Code Example: Principal Component Analysis</h2><p>Here&rsquo;s a Python implementation of PCA using eigenvalue decomposition:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>pca</span>(X, num_components):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Perform PCA on dataset X
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Parameters:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    X: Dataset, shape (n_samples, n_features)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    num_components: Number of principal components to keep
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    X_pca: Data transformed into principal components
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    components: Principal components
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    explained_variance: Explained variance of each component
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Center the data</span>
</span></span><span style=display:flex><span>    X_centered <span style=color:#f92672>=</span> X <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>mean(X, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Compute covariance matrix</span>
</span></span><span style=display:flex><span>    cov_matrix <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>cov(X_centered, rowvar<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Eigenvalue decomposition</span>
</span></span><span style=display:flex><span>    eigenvalues, eigenvectors <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>eigh(cov_matrix)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Sort eigenvalues and corresponding eigenvectors in descending order</span>
</span></span><span style=display:flex><span>    idx <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>argsort(eigenvalues)[::<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    eigenvalues <span style=color:#f92672>=</span> eigenvalues[idx]
</span></span><span style=display:flex><span>    eigenvectors <span style=color:#f92672>=</span> eigenvectors[:, idx]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Select top k eigenvectors</span>
</span></span><span style=display:flex><span>    components <span style=color:#f92672>=</span> eigenvectors[:, :num_components]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Compute explained variance</span>
</span></span><span style=display:flex><span>    explained_variance <span style=color:#f92672>=</span> eigenvalues[:num_components] <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>sum(eigenvalues)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Project data onto principal components</span>
</span></span><span style=display:flex><span>    X_pca <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(X_centered, components)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> X_pca, components, explained_variance
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Example usage</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    <span style=color:#75715e># Generate random data</span>
</span></span><span style=display:flex><span>    np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>seed(<span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>    X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Apply PCA</span>
</span></span><span style=display:flex><span>    X_pca, components, explained_variance <span style=color:#f92672>=</span> pca(X, num_components<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Print results</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Explained variance: </span><span style=color:#e6db74>{</span>explained_variance<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Principal components shape: </span><span style=color:#e6db74>{</span>components<span style=color:#f92672>.</span>shape<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Plot results</span>
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>scatter(X_pca[:, <span style=color:#ae81ff>0</span>], X_pca[:, <span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#34;First Principal Component&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#34;Second Principal Component&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;PCA Example&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>grid(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><h2 id=assignment>Assignment</h2><p>Implement the following algorithms using the mathematical concepts covered in this module:</p><ol><li>Linear regression using normal equations: $\hat{\beta} = (X^T X)^{-1} X^T y$</li><li>k-means clustering</li><li>Naive Bayes classifier using Bayes&rsquo; theorem</li></ol><h2 id=resources>Resources</h2><ul><li>Textbook: &ldquo;Mathematics for Machine Learning&rdquo; by Deisenroth, Faisal, and Ong</li><li>Online course: MIT OpenCourseWare 18.06 Linear Algebra</li><li>Python libraries: NumPy, SciPy, scikit-learn</li></ul></div></article><style>.course-page{max-width:800px;margin:0 auto}.course-header{margin-bottom:2rem;padding-bottom:1.5rem;border-bottom:1px solid var(--border)}.course-title{margin-bottom:1.2rem}.course-meta{display:flex;flex-wrap:wrap;gap:1.2rem;margin-bottom:1.2rem}.course-detail{display:flex;align-items:center;font-size:.95rem}.detail-label{font-weight:600;margin-right:.5rem;color:var(--secondary)}.detail-value{color:var(--primary)}.detail-value.semester,.detail-value.level{padding:.2rem .5rem;border-radius:4px;font-size:.85rem}.detail-value.semester{background-color:var(--primary);color:var(--theme)}.detail-value.level{background-color:var(--tertiary);color:var(--primary)}.course-links{display:flex;flex-wrap:wrap;gap:1rem;margin-bottom:1.5rem}.course-link{display:inline-flex;align-items:center;gap:.5rem;padding:.4rem .8rem;font-size:.9rem;border-radius:4px;text-decoration:none;transition:opacity .2s}.syllabus-link{background-color:var(--primary);color:var(--theme)}.website-link{background-color:var(--tertiary);color:var(--primary)}.course-link:hover{opacity:.85}.course-summary{font-size:1.1rem;line-height:1.5;padding:1rem;background-color:var(--code-bg);border-radius:5px;border-left:4px solid var(--primary)}.course-content{line-height:1.6;font-size:1.05rem}.course-content h2{margin-top:2rem;margin-bottom:1rem;padding-bottom:.3rem;border-bottom:1px solid var(--border)}.course-content h3{margin-top:1.5rem;margin-bottom:.8rem}.course-content ul,.course-content ol{padding-left:1.5rem;margin-bottom:1.5rem}.course-content li{margin-bottom:.5rem}.course-content table{width:100%;border-collapse:collapse;margin-bottom:1.5rem}.course-content th{text-align:left;padding:.8rem;background-color:var(--code-bg);border-bottom:2px solid var(--border)}.course-content td{padding:.8rem;border-bottom:1px solid var(--border)}.course-content tr:nth-child(even){background-color:var(--code-bg)}.course-content code{background-color:var(--code-bg);padding:.2em .4em;border-radius:3px;font-size:.9em}.course-content pre{background-color:var(--code-bg);padding:1rem;border-radius:5px;overflow:auto;margin-bottom:1.5rem}.course-content blockquote{margin:1.5rem 0;padding:1rem 1.5rem;border-left:4px solid var(--primary);background-color:var(--code-bg);font-style:italic}@media(max-width:768px){.course-meta{flex-direction:column;gap:.8rem}}</style></main><footer class=footer><span>&copy; 2025 <a href=https://yourname.github.io/>Yusuf Evirgen</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>